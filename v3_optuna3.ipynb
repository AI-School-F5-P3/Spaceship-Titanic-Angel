{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Importando las librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Importando las librerías para la selección de características\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos de entrenamiento y prueba\n",
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8693, 14)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>HomePlanet</th>\n",
       "      <th>CryoSleep</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Age</th>\n",
       "      <th>VIP</th>\n",
       "      <th>RoomService</th>\n",
       "      <th>FoodCourt</th>\n",
       "      <th>ShoppingMall</th>\n",
       "      <th>Spa</th>\n",
       "      <th>VRDeck</th>\n",
       "      <th>Name</th>\n",
       "      <th>Transported</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0001_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>B/0/P</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>39.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Maham Ofracculy</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0002_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>24.0</td>\n",
       "      <td>False</td>\n",
       "      <td>109.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>549.0</td>\n",
       "      <td>44.0</td>\n",
       "      <td>Juanna Vines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0003_01</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>58.0</td>\n",
       "      <td>True</td>\n",
       "      <td>43.0</td>\n",
       "      <td>3576.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6715.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>Altark Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003_02</td>\n",
       "      <td>Europa</td>\n",
       "      <td>False</td>\n",
       "      <td>A/0/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>33.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1283.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>3329.0</td>\n",
       "      <td>193.0</td>\n",
       "      <td>Solam Susent</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004_01</td>\n",
       "      <td>Earth</td>\n",
       "      <td>False</td>\n",
       "      <td>F/1/S</td>\n",
       "      <td>TRAPPIST-1e</td>\n",
       "      <td>16.0</td>\n",
       "      <td>False</td>\n",
       "      <td>303.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>151.0</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Willy Santantines</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
       "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
       "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
       "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
       "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
       "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
       "\n",
       "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
       "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
       "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
       "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
       "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
       "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
       "\n",
       "   Transported  \n",
       "0        False  \n",
       "1         True  \n",
       "2        False  \n",
       "3        False  \n",
       "4         True  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8693 entries, 0 to 8692\n",
      "Data columns (total 14 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   PassengerId   8693 non-null   object \n",
      " 1   HomePlanet    8492 non-null   object \n",
      " 2   CryoSleep     8476 non-null   object \n",
      " 3   Cabin         8494 non-null   object \n",
      " 4   Destination   8511 non-null   object \n",
      " 5   Age           8514 non-null   float64\n",
      " 6   VIP           8490 non-null   object \n",
      " 7   RoomService   8512 non-null   float64\n",
      " 8   FoodCourt     8510 non-null   float64\n",
      " 9   ShoppingMall  8485 non-null   float64\n",
      " 10  Spa           8510 non-null   float64\n",
      " 11  VRDeck        8505 non-null   float64\n",
      " 12  Name          8493 non-null   object \n",
      " 13  Transported   8693 non-null   bool   \n",
      "dtypes: bool(1), float64(6), object(7)\n",
      "memory usage: 891.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7826\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# ======== Ingeniería de características ========\n",
    "\n",
    "# 1. Grupos de edad\n",
    "def categorize_age(age):\n",
    "    if pd.isna(age):\n",
    "        return 'Unknown'\n",
    "    elif age < 18:\n",
    "        return 'Child'\n",
    "    elif age < 30:\n",
    "        return 'Young Adult'\n",
    "    elif age < 60:\n",
    "        return 'Adult'\n",
    "    else:\n",
    "        return 'Senior'\n",
    "    \n",
    "data['AgeGroup'] = data['Age'].apply(categorize_age)\n",
    "\n",
    "# 2. PassengerId -> group y group_size\n",
    "data['PassengerId'] = data['PassengerId'].astype(str)\n",
    "data['Group'] = data['PassengerId'].apply(lambda x: x.split('_')[0])\n",
    "data['Group_size'] = data.groupby('Group')['Group'].transform('count')\n",
    "\n",
    "# 3. Cabin -> Deck, Number, Side\n",
    "def split_cabin(cabin):\n",
    "    if isinstance(cabin, str):  # Solo procesar si es una cadena\n",
    "        parts = cabin.split('/')\n",
    "        deck = parts[0] if len(parts) > 0 else 'Unknown'\n",
    "        number = parts[1] if len(parts) > 1 else 'Unknown'\n",
    "        side = parts[2] if len(parts) > 2 else 'Unknown'\n",
    "        return pd.Series([deck, number, side])\n",
    "    else:\n",
    "        return pd.Series(['Unknown', 'Unknown', 'Unknown'])\n",
    "\n",
    "data[['CabinDeck', 'CabinNumber', 'CabinSide']] = data['Cabin'].apply(split_cabin)\n",
    "\n",
    "# 4. Name -> Surname\n",
    "data['Surname'] = data['Name'].apply(lambda x: x.split()[-1] if isinstance(x, str) else 'Unknown')\n",
    "\n",
    "\n",
    "# ======== Imputación avanzada de valores faltantes ========\n",
    "\n",
    "# Crear función para imputar usando Group\n",
    "def impute_with_group(feature):\n",
    "    data[feature] = data.groupby('Group')[feature].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else np.nan))\n",
    "\n",
    "# Imputar valores faltantes para HomePlanet\n",
    "impute_with_group('HomePlanet')\n",
    "\n",
    "# Si aún hay valores faltantes en HomePlanet, usar CabinDeck\n",
    "data['HomePlanet'] = data.groupby('CabinDeck')['HomePlanet'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Unknown'))\n",
    "\n",
    "# Imputar valores faltantes para Destination usando HomePlanet\n",
    "data['Destination'] = data.groupby('HomePlanet')['Destination'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Unknown'))\n",
    "\n",
    "# Imputar valores faltantes para CryoSleep y VIP usando Group y Surname\n",
    "impute_with_group('CryoSleep')\n",
    "impute_with_group('VIP')\n",
    "\n",
    "# Imputar valores faltantes de CabinSize usando Group o Surname\n",
    "data['CabinSize'] = data.groupby('Surname')['CabinNumber'].transform(lambda x: x.fillna(x.mode()[0] if not x.mode().empty else 'Unknown'))\n",
    "\n",
    "# Para otras características, simplemente utilizamos imputación media o más lógica\n",
    "imputer = SimpleImputer(strategy='most_frequent')\n",
    "data[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']] = imputer.fit_transform(data[['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']])\n",
    "\n",
    "# ======== Codificación de variables categóricas ========\n",
    "continuous_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Codificar las variables categóricas utilizando OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n",
    "encoded_features = encoder.fit_transform(data[categorical_cols])\n",
    "\n",
    "# Añadir estas nuevas características al dataset\n",
    "df_encoded = pd.DataFrame(encoded_features, columns=encoder.get_feature_names_out(categorical_cols))\n",
    "data = pd.concat([data, df_encoded], axis=1)\n",
    "\n",
    "# Eliminar columnas originales innecesarias\n",
    "data = data.drop(columns=categorical_cols.tolist() + ['PassengerId', 'Cabin', 'Name'])\n",
    "\n",
    "# ======== Entrenamiento del modelo ========\n",
    "\n",
    "# Dividir los datos en train y test\n",
    "X = data.drop(columns=['Transported'])  # Features\n",
    "y = data['Transported']  # Target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entrenar el modelo\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Hacer predicciones y evaluar el modelo\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.impute import SimpleImputer\n",
    "\n",
    "# # Identificar características continuas y categóricas\n",
    "# continuous_cols = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "# categorical_cols = data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# # Imputación de la mediana para las características continuas\n",
    "# imputer_median = SimpleImputer(strategy='median')\n",
    "# data[continuous_cols] = imputer_median.fit_transform(data[continuous_cols])\n",
    "\n",
    "# # Imputación de la moda para las características categóricas\n",
    "# imputer_mode = SimpleImputer(strategy='most_frequent')\n",
    "# data[categorical_cols] = imputer_mode.fit_transform(data[categorical_cols])\n",
    "\n",
    "# # Eliminar la columna PassengerId ya que es un identificador único\n",
    "# # data.drop('PassengerId', inplace=True, axis=1)\n",
    "\n",
    "# # Convertir las columnas mixtas en tipo string (si aplica)\n",
    "# for col in data.columns:\n",
    "#     if data[col].dtype == 'object':\n",
    "#         data[col] = data[col].astype(str)\n",
    "\n",
    "# # Aplicar Label Encoding a las columnas categóricas\n",
    "# oe = {}\n",
    "# for col in categorical_cols:\n",
    "#     oe[col] = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n",
    "#     data[col] = oe[col].fit_transform(data[col].values.reshape(-1, 1))\n",
    "\n",
    "# # Separar características (X) y la variable objetivo (y)\n",
    "# X = data.drop('Transported', axis=1)\n",
    "# y = data['Transported']\n",
    "\n",
    "# # Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(((6954, 17406), (6954,)), ((1739, 17406), (1739,)))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(X_train.shape, y_train.shape), (X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 18:14:15,334] A new study created in memory with name: no-name-5013b34c-0fa5-4ba4-a6a9-c9dfaaf7271c\n",
      "[I 2024-09-23 18:14:17,033] Trial 0 finished with value: 0.679700977573318 and parameters: {'n_estimators': 61, 'max_depth': 21, 'min_samples_leaf': 10}. Best is trial 0 with value: 0.679700977573318.\n",
      "[I 2024-09-23 18:14:18,962] Trial 1 finished with value: 0.6820011500862565 and parameters: {'n_estimators': 82, 'max_depth': 19, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.6820011500862565.\n",
      "[I 2024-09-23 18:14:21,152] Trial 2 finished with value: 0.6762507188039103 and parameters: {'n_estimators': 179, 'max_depth': 19, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.6820011500862565.\n",
      "[I 2024-09-23 18:14:23,057] Trial 3 finished with value: 0.679700977573318 and parameters: {'n_estimators': 130, 'max_depth': 8, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.6820011500862565.\n",
      "[I 2024-09-23 18:14:24,808] Trial 4 finished with value: 0.6768257619321449 and parameters: {'n_estimators': 75, 'max_depth': 5, 'min_samples_leaf': 8}. Best is trial 1 with value: 0.6820011500862565.\n",
      "[I 2024-09-23 18:14:26,683] Trial 5 finished with value: 0.679700977573318 and parameters: {'n_estimators': 60, 'max_depth': 16, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.6820011500862565.\n",
      "[I 2024-09-23 18:14:28,774] Trial 6 finished with value: 0.6768257619321449 and parameters: {'n_estimators': 154, 'max_depth': 16, 'min_samples_leaf': 3}. Best is trial 1 with value: 0.6820011500862565.\n",
      "[I 2024-09-23 18:14:30,706] Trial 7 finished with value: 0.679700977573318 and parameters: {'n_estimators': 69, 'max_depth': 3, 'min_samples_leaf': 2}. Best is trial 1 with value: 0.6820011500862565.\n",
      "[I 2024-09-23 18:14:32,558] Trial 8 finished with value: 0.6820011500862565 and parameters: {'n_estimators': 94, 'max_depth': 8, 'min_samples_leaf': 7}. Best is trial 1 with value: 0.6820011500862565.\n",
      "[I 2024-09-23 18:14:38,472] Trial 9 finished with value: 0.745830937320299 and parameters: {'n_estimators': 71, 'max_depth': 29, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:14:40,359] Trial 10 finished with value: 0.6814261069580219 and parameters: {'n_estimators': 112, 'max_depth': 29, 'min_samples_leaf': 5}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:14:42,158] Trial 11 finished with value: 0.6820011500862565 and parameters: {'n_estimators': 95, 'max_depth': 30, 'min_samples_leaf': 5}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:14:43,974] Trial 12 finished with value: 0.6820011500862565 and parameters: {'n_estimators': 90, 'max_depth': 23, 'min_samples_leaf': 9}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:14:45,638] Trial 13 finished with value: 0.6756756756756757 and parameters: {'n_estimators': 50, 'max_depth': 25, 'min_samples_leaf': 7}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:14:47,643] Trial 14 finished with value: 0.679700977573318 and parameters: {'n_estimators': 125, 'max_depth': 13, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:14:49,406] Trial 15 finished with value: 0.6820011500862565 and parameters: {'n_estimators': 82, 'max_depth': 24, 'min_samples_leaf': 7}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:14:57,039] Trial 16 finished with value: 0.7429557216791259 and parameters: {'n_estimators': 111, 'max_depth': 27, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:14:59,071] Trial 17 finished with value: 0.6768257619321449 and parameters: {'n_estimators': 153, 'max_depth': 27, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:06,831] Trial 18 finished with value: 0.7429557216791259 and parameters: {'n_estimators': 111, 'max_depth': 27, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:20,602] Trial 19 finished with value: 0.7423806785508913 and parameters: {'n_estimators': 191, 'max_depth': 30, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:22,648] Trial 20 finished with value: 0.679700977573318 and parameters: {'n_estimators': 138, 'max_depth': 27, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:30,297] Trial 21 finished with value: 0.7423806785508913 and parameters: {'n_estimators': 109, 'max_depth': 27, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:36,828] Trial 22 finished with value: 0.7441058079355952 and parameters: {'n_estimators': 111, 'max_depth': 22, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:38,714] Trial 23 finished with value: 0.679700977573318 and parameters: {'n_estimators': 120, 'max_depth': 22, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:47,330] Trial 24 finished with value: 0.7452558941920644 and parameters: {'n_estimators': 141, 'max_depth': 25, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:54,731] Trial 25 finished with value: 0.7406555491661875 and parameters: {'n_estimators': 147, 'max_depth': 20, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:56,684] Trial 26 finished with value: 0.679700977573318 and parameters: {'n_estimators': 136, 'max_depth': 25, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:15:58,773] Trial 27 finished with value: 0.6768257619321449 and parameters: {'n_estimators': 157, 'max_depth': 18, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:00,960] Trial 28 finished with value: 0.6756756756756757 and parameters: {'n_estimators': 169, 'max_depth': 23, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:02,539] Trial 29 finished with value: 0.6756756756756757 and parameters: {'n_estimators': 53, 'max_depth': 21, 'min_samples_leaf': 10}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:08,491] Trial 30 finished with value: 0.7418056354226567 and parameters: {'n_estimators': 167, 'max_depth': 13, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:15,871] Trial 31 finished with value: 0.7452558941920644 and parameters: {'n_estimators': 105, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:17,687] Trial 32 finished with value: 0.6802760207015526 and parameters: {'n_estimators': 104, 'max_depth': 29, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:25,203] Trial 33 finished with value: 0.7446808510638298 and parameters: {'n_estimators': 121, 'max_depth': 25, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:27,205] Trial 34 finished with value: 0.679700977573318 and parameters: {'n_estimators': 141, 'max_depth': 25, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:29,106] Trial 35 finished with value: 0.6785508913168488 and parameters: {'n_estimators': 124, 'max_depth': 28, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:30,785] Trial 36 finished with value: 0.6768257619321449 and parameters: {'n_estimators': 76, 'max_depth': 25, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:40,335] Trial 37 finished with value: 0.7441058079355952 and parameters: {'n_estimators': 133, 'max_depth': 30, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:42,135] Trial 38 finished with value: 0.6802760207015526 and parameters: {'n_estimators': 99, 'max_depth': 18, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:43,781] Trial 39 finished with value: 0.679700977573318 and parameters: {'n_estimators': 67, 'max_depth': 26, 'min_samples_leaf': 6}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:45,559] Trial 40 finished with value: 0.6820011500862565 and parameters: {'n_estimators': 84, 'max_depth': 29, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:52,512] Trial 41 finished with value: 0.7423806785508913 and parameters: {'n_estimators': 120, 'max_depth': 23, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:16:58,921] Trial 42 finished with value: 0.7423806785508913 and parameters: {'n_estimators': 117, 'max_depth': 21, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:05,287] Trial 43 finished with value: 0.7412305922944221 and parameters: {'n_estimators': 102, 'max_depth': 24, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:07,197] Trial 44 finished with value: 0.679700977573318 and parameters: {'n_estimators': 127, 'max_depth': 28, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:08,944] Trial 45 finished with value: 0.6820011500862565 and parameters: {'n_estimators': 89, 'max_depth': 22, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:14,806] Trial 46 finished with value: 0.7429557216791259 and parameters: {'n_estimators': 144, 'max_depth': 15, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:16,425] Trial 47 finished with value: 0.679700977573318 and parameters: {'n_estimators': 61, 'max_depth': 26, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:18,397] Trial 48 finished with value: 0.679700977573318 and parameters: {'n_estimators': 130, 'max_depth': 19, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:20,140] Trial 49 finished with value: 0.6768257619321449 and parameters: {'n_estimators': 75, 'max_depth': 28, 'min_samples_leaf': 8}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:27,194] Trial 50 finished with value: 0.7412305922944221 and parameters: {'n_estimators': 115, 'max_depth': 24, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:36,878] Trial 51 finished with value: 0.7441058079355952 and parameters: {'n_estimators': 133, 'max_depth': 30, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:44,595] Trial 52 finished with value: 0.7429557216791259 and parameters: {'n_estimators': 106, 'max_depth': 29, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:56,131] Trial 53 finished with value: 0.7441058079355952 and parameters: {'n_estimators': 162, 'max_depth': 30, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:17:57,924] Trial 54 finished with value: 0.6802760207015526 and parameters: {'n_estimators': 98, 'max_depth': 26, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:07,017] Trial 55 finished with value: 0.7452558941920644 and parameters: {'n_estimators': 133, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:09,017] Trial 56 finished with value: 0.6785508913168488 and parameters: {'n_estimators': 147, 'max_depth': 28, 'min_samples_leaf': 3}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:10,822] Trial 57 finished with value: 0.6820011500862565 and parameters: {'n_estimators': 90, 'max_depth': 26, 'min_samples_leaf': 4}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:12,723] Trial 58 finished with value: 0.6785508913168488 and parameters: {'n_estimators': 124, 'max_depth': 24, 'min_samples_leaf': 9}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:14,590] Trial 59 finished with value: 0.6814261069580219 and parameters: {'n_estimators': 114, 'max_depth': 8, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:24,053] Trial 60 finished with value: 0.7418056354226567 and parameters: {'n_estimators': 181, 'max_depth': 22, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:33,162] Trial 61 finished with value: 0.7452558941920644 and parameters: {'n_estimators': 133, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:42,863] Trial 62 finished with value: 0.7395054629097182 and parameters: {'n_estimators': 151, 'max_depth': 27, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:52,104] Trial 63 finished with value: 0.745830937320299 and parameters: {'n_estimators': 137, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:18:54,060] Trial 64 finished with value: 0.679700977573318 and parameters: {'n_estimators': 137, 'max_depth': 28, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:19:03,896] Trial 65 finished with value: 0.7423806785508913 and parameters: {'n_estimators': 142, 'max_depth': 29, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:19:05,862] Trial 66 finished with value: 0.679700977573318 and parameters: {'n_estimators': 128, 'max_depth': 27, 'min_samples_leaf': 6}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:19:13,596] Trial 67 finished with value: 0.7441058079355952 and parameters: {'n_estimators': 120, 'max_depth': 25, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:19:15,703] Trial 68 finished with value: 0.6768257619321449 and parameters: {'n_estimators': 158, 'max_depth': 29, 'min_samples_leaf': 2}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:19:25,000] Trial 69 finished with value: 0.7435307648073606 and parameters: {'n_estimators': 149, 'max_depth': 26, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:19:34,589] Trial 70 finished with value: 0.7441058079355952 and parameters: {'n_estimators': 133, 'max_depth': 30, 'min_samples_leaf': 1}. Best is trial 9 with value: 0.745830937320299.\n",
      "[I 2024-09-23 18:19:42,373] Trial 71 finished with value: 0.7464059804485337 and parameters: {'n_estimators': 109, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 71 with value: 0.7464059804485337.\n",
      "[I 2024-09-23 18:19:51,857] Trial 72 finished with value: 0.7475560667050029 and parameters: {'n_estimators': 141, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:19:53,866] Trial 73 finished with value: 0.679700977573318 and parameters: {'n_estimators': 140, 'max_depth': 28, 'min_samples_leaf': 2}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:03,866] Trial 74 finished with value: 0.7429557216791259 and parameters: {'n_estimators': 144, 'max_depth': 29, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:13,739] Trial 75 finished with value: 0.7400805060379528 and parameters: {'n_estimators': 154, 'max_depth': 27, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:15,704] Trial 76 finished with value: 0.679700977573318 and parameters: {'n_estimators': 137, 'max_depth': 30, 'min_samples_leaf': 2}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:24,717] Trial 77 finished with value: 0.7452558941920644 and parameters: {'n_estimators': 133, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:26,365] Trial 78 finished with value: 0.679700977573318 and parameters: {'n_estimators': 69, 'max_depth': 27, 'min_samples_leaf': 5}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:28,194] Trial 79 finished with value: 0.6814261069580219 and parameters: {'n_estimators': 107, 'max_depth': 29, 'min_samples_leaf': 3}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:36,428] Trial 80 finished with value: 0.7435307648073606 and parameters: {'n_estimators': 130, 'max_depth': 26, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:45,476] Trial 81 finished with value: 0.7446808510638298 and parameters: {'n_estimators': 134, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:47,411] Trial 82 finished with value: 0.6785508913168488 and parameters: {'n_estimators': 124, 'max_depth': 28, 'min_samples_leaf': 2}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:20:56,837] Trial 83 finished with value: 0.7423806785508913 and parameters: {'n_estimators': 146, 'max_depth': 27, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:06,567] Trial 84 finished with value: 0.7429557216791259 and parameters: {'n_estimators': 139, 'max_depth': 29, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:08,536] Trial 85 finished with value: 0.679700977573318 and parameters: {'n_estimators': 142, 'max_depth': 30, 'min_samples_leaf': 2}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:13,162] Trial 86 finished with value: 0.7429557216791259 and parameters: {'n_estimators': 129, 'max_depth': 12, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:15,087] Trial 87 finished with value: 0.679700977573318 and parameters: {'n_estimators': 116, 'max_depth': 26, 'min_samples_leaf': 2}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:24,192] Trial 88 finished with value: 0.745830937320299 and parameters: {'n_estimators': 135, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:26,488] Trial 89 finished with value: 0.7412305922944221 and parameters: {'n_estimators': 94, 'max_depth': 4, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:36,495] Trial 90 finished with value: 0.7406555491661875 and parameters: {'n_estimators': 156, 'max_depth': 27, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:45,569] Trial 91 finished with value: 0.7446808510638298 and parameters: {'n_estimators': 134, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:54,548] Trial 92 finished with value: 0.7452558941920644 and parameters: {'n_estimators': 131, 'max_depth': 28, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:21:56,501] Trial 93 finished with value: 0.679700977573318 and parameters: {'n_estimators': 125, 'max_depth': 29, 'min_samples_leaf': 2}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:22:04,155] Trial 94 finished with value: 0.7446808510638298 and parameters: {'n_estimators': 121, 'max_depth': 25, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:22:14,860] Trial 95 finished with value: 0.7435307648073606 and parameters: {'n_estimators': 150, 'max_depth': 30, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:22:16,752] Trial 96 finished with value: 0.6820011500862565 and parameters: {'n_estimators': 86, 'max_depth': 29, 'min_samples_leaf': 2}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:22:26,075] Trial 97 finished with value: 0.7423806785508913 and parameters: {'n_estimators': 139, 'max_depth': 26, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:22:28,159] Trial 98 finished with value: 0.679700977573318 and parameters: {'n_estimators': 127, 'max_depth': 27, 'min_samples_leaf': 2}. Best is trial 72 with value: 0.7475560667050029.\n",
      "[I 2024-09-23 18:22:36,799] Trial 99 finished with value: 0.7418056354226567 and parameters: {'n_estimators': 145, 'max_depth': 23, 'min_samples_leaf': 1}. Best is trial 72 with value: 0.7475560667050029.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'n_estimators': 141, 'max_depth': 28, 'min_samples_leaf': 1}\n",
      "Best Accuracy: 0.7476\n"
     ]
    }
   ],
   "source": [
    "# Definir la función de objetivo\n",
    "def objective(trial):\n",
    "    # Hiperparámetros a optimizar\n",
    "    n_estimators = trial.suggest_int('n_estimators', 50, 200)\n",
    "    max_depth = trial.suggest_int('max_depth', 3, 30)\n",
    "    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 10)\n",
    "\n",
    "    # Entrenar el modelo\n",
    "    clf = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, min_samples_leaf=min_samples_leaf, random_state=42)\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Hacer predicciones y evaluar el modelo\n",
    "    y_pred = clf.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Ejecutar la optimización\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100)\n",
    "\n",
    "# Mejor combinación de hiperparámetros\n",
    "best_params = study.best_params\n",
    "print(f'Best hyperparameters: {best_params}')\n",
    "\n",
    "# Entrenar el modelo con los mejores hiperparámetros\n",
    "best_clf = RandomForestClassifier(**best_params, random_state=42)\n",
    "best_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluar el modelo optimizado\n",
    "y_pred_best = best_clf.predict(X_test)\n",
    "best_accuracy = accuracy_score(y_test, y_pred_best)\n",
    "print(f'Best Accuracy: {best_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Defining the objective function for Optuna with feature selection\n",
    "    \"\"\"\n",
    "    n_estimators = trial.suggest_int('n_estimators', 100, 1000)\n",
    "    max_depth = trial.suggest_int('max_depth', 2, 32)\n",
    "    # Number of features to select\n",
    "    k_best = trial.suggest_int('k_best', 1, X_train.shape[1])  \n",
    "\n",
    "    # Feature selection\n",
    "    select_k_best = SelectKBest(score_func=f_classif, k=k_best) # SelectKBest: Esta clase se utiliza para seleccionar las k mejores características según un criterio de puntuación. En este caso, f_classif se utiliza como la función de puntuación, que es adecuada para clasificación.\n",
    "    preprocessor = ColumnTransformer([('select', select_k_best, X.columns)], remainder='passthrough') # Se define un transformador de columnas que aplica la selección de características a las columnas especificadas en X.columns. El parámetro remainder='passthrough' asegura que las columnas no seleccionadas se mantengan sin cambios.\n",
    "    \n",
    "    # Building the model pipeline\n",
    "    clf = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          ('classifier', RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42))]) # Se crea un pipeline que encadena el preprocesador y el clasificador. Esto simplifica el proceso de entrenamiento y predicción, asegurando que las transformaciones se apliquen correctamente. RandomForestClassifier: Se inicializa el clasificador con los hiperparámetros sugeridos (número de estimadores y profundidad máxima).\n",
    "\n",
    "    clf.fit(X_train, y_train) # Se entrena el modelo (clf) usando los datos de entrenamiento (X_train, y_train).\n",
    "    y_pred = clf.predict(X_test) # Se realizan predicciones sobre el conjunto de prueba (X_test).\n",
    "    accuracy = accuracy_score(y_test, y_pred) # Se calcula la precisión del modelo utilizando la función accuracy_score, comparando las predicciones (y_pred) con las etiquetas verdaderas (y_test).\n",
    "    \n",
    "    # Storing the trained classifier in the study object\n",
    "    trial.set_user_attr('classifier', clf) # almacena el clasificador entrenado en el objeto trial. Esto puede ser útil para análisis posteriores o para evaluar el rendimiento del modelo después de la optimización\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-09-23 18:10:33,174] A new study created in memory with name: best_features_and_hyperparameters\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[I 2024-09-23 18:10:43,975] Trial 0 finished with value: 0.7492811960897068 and parameters: {'n_estimators': 287, 'max_depth': 6, 'k_best': 8751}. Best is trial 0 with value: 0.7492811960897068.\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[I 2024-09-23 18:11:05,657] Trial 1 finished with value: 0.7389304197814837 and parameters: {'n_estimators': 905, 'max_depth': 7, 'k_best': 14517}. Best is trial 0 with value: 0.7492811960897068.\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[I 2024-09-23 18:11:18,296] Trial 2 finished with value: 0.7412305922944221 and parameters: {'n_estimators': 370, 'max_depth': 6, 'k_best': 14745}. Best is trial 0 with value: 0.7492811960897068.\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[I 2024-09-23 18:11:36,330] Trial 3 finished with value: 0.7412305922944221 and parameters: {'n_estimators': 424, 'max_depth': 15, 'k_best': 9136}. Best is trial 0 with value: 0.7492811960897068.\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[I 2024-09-23 18:12:04,409] Trial 4 finished with value: 0.7446808510638298 and parameters: {'n_estimators': 850, 'max_depth': 16, 'k_best': 7635}. Best is trial 0 with value: 0.7492811960897068.\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[I 2024-09-23 18:12:19,734] Trial 5 finished with value: 0.7412305922944221 and parameters: {'n_estimators': 346, 'max_depth': 9, 'k_best': 15644}. Best is trial 0 with value: 0.7492811960897068.\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[I 2024-09-23 18:12:28,641] Trial 6 finished with value: 0.7239792984473835 and parameters: {'n_estimators': 601, 'max_depth': 2, 'k_best': 7218}. Best is trial 0 with value: 0.7492811960897068.\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[I 2024-09-23 18:12:42,619] Trial 7 finished with value: 0.7366302472685451 and parameters: {'n_estimators': 369, 'max_depth': 8, 'k_best': 16209}. Best is trial 0 with value: 0.7492811960897068.\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[I 2024-09-23 18:12:58,716] Trial 8 finished with value: 0.7435307648073606 and parameters: {'n_estimators': 923, 'max_depth': 10, 'k_best': 3068}. Best is trial 0 with value: 0.7492811960897068.\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:112: UserWarning: Features [   10    11    14 ... 17403 17404 17405] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/feature_selection/_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "[W 2024-09-23 18:13:48,892] Trial 9 failed with parameters: {'n_estimators': 941, 'max_depth': 30, 'k_best': 11724} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "                      ^^^^^^^^^^^\n",
      "  File \"/var/folders/6l/xc4cvh3j3zj9b4f6zl1r0m100000gn/T/ipykernel_53203/720641201.py\", line 18, in objective\n",
      "    clf.fit(X_train, y_train) # Se entrena el modelo (clf) usando los datos de entrenamiento (X_train, y_train).\n",
      "    ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/pipeline.py\", line 473, in fit\n",
      "    self._final_estimator.fit(Xt, y, **last_step_params[\"fit\"])\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/base.py\", line 1473, in wrapper\n",
      "    return fit_method(estimator, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/ensemble/_forest.py\", line 489, in fit\n",
      "    trees = Parallel(\n",
      "            ^^^^^^^^^\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 74, in __call__\n",
      "    return super().__call__(iterable_with_config)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/joblib/parallel.py\", line 1918, in __call__\n",
      "    return output if self.return_generator else list(output)\n",
      "                                                ^^^^^^^^^^^^\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/joblib/parallel.py\", line 1847, in _get_sequential_output\n",
      "    res = func(*args, **kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/utils/parallel.py\", line 136, in __call__\n",
      "    return self.function(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/ensemble/_forest.py\", line 192, in _parallel_build_trees\n",
      "    tree._fit(\n",
      "  File \"/Users/cash/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/tree/_classes.py\", line 472, in _fit\n",
      "    builder.build(self.tree_, X, y, sample_weight, missing_values_in_feature_mask)\n",
      "KeyboardInterrupt\n",
      "[W 2024-09-23 18:13:48,894] Trial 9 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Running the optimization\u001b[39;00m\n\u001b[1;32m      2\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(study_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_features_and_hyperparameters\u001b[39m\u001b[38;5;124m'\u001b[39m, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;66;03m# optuna.create_study(...) es una función que se utiliza para crear un nuevo objeto de estudio. Un estudio en Optuna es un contenedor para todas las pruebas (trials) que se realizarán durante la optimización. Tiene dos parámetros: 1. study_name='best_features_and_hyperparameters': Este es el nombre que se le asigna al estudio. Puedes usar este nombre para identificar el estudio en la base de datos de Optuna, especialmente si estás guardando resultados o usando la interfaz de visualización. 2. direction='maximize': Este parámetro indica que el objetivo de la optimización es maximizar el valor que se devolverá en la función objetivo (en este caso, la precisión del modelo). Si en lugar de maximizar quisieras minimizar un valor (como la pérdida), usarías direction='minimize'.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# study.optimize(...): es un método que ejecuta el proceso de optimización, llamando repetidamente a la función objetivo para encontrar la mejor combinación de hiperparámetros y características. Tiene dos parámetros: 1. objective: Este es el nombre de la función objetivo que se definió anteriormente. Optuna la llamará múltiples veces con diferentes configuraciones de hiperparámetros, que se generan automáticamente. 2. n_trials=50: Este parámetro indica el número total de pruebas (trials) que se realizarán. En cada prueba, Optuna selecciona un conjunto diferente de hiperparámetros y características que serán probados en el modelo. En este caso, se realizarán 50 pruebas diferentes.\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[21], line 18\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Building the model pipeline\u001b[39;00m\n\u001b[1;32m     15\u001b[0m clf \u001b[38;5;241m=\u001b[39m Pipeline(steps\u001b[38;5;241m=\u001b[39m[(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpreprocessor\u001b[39m\u001b[38;5;124m'\u001b[39m, preprocessor),\n\u001b[1;32m     16\u001b[0m                       (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclassifier\u001b[39m\u001b[38;5;124m'\u001b[39m, RandomForestClassifier(n_estimators\u001b[38;5;241m=\u001b[39mn_estimators, max_depth\u001b[38;5;241m=\u001b[39mmax_depth, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m))]) \u001b[38;5;66;03m# Se crea un pipeline que encadena el preprocesador y el clasificador. Esto simplifica el proceso de entrenamiento y predicción, asegurando que las transformaciones se apliquen correctamente. RandomForestClassifier: Se inicializa el clasificador con los hiperparámetros sugeridos (número de estimadores y profundidad máxima).\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Se entrena el modelo (clf) usando los datos de entrenamiento (X_train, y_train).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test) \u001b[38;5;66;03m# Se realizan predicciones sobre el conjunto de prueba (X_test).\u001b[39;00m\n\u001b[1;32m     20\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred) \u001b[38;5;66;03m# Se calcula la precisión del modelo utilizando la función accuracy_score, comparando las predicciones (y_pred) con las etiquetas verdaderas (y_test).\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[0;34m(self, X, y, **params)\u001b[0m\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[0;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/utils/parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     73\u001b[0m )\n\u001b[0;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/joblib/parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/joblib/parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/utils/parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/Desktop/F5/Python/Machine_Learning/non-supervised-learning/env/lib/python3.12/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Running the optimization\n",
    "study = optuna.create_study(study_name='best_features_and_hyperparameters', direction='maximize') # optuna.create_study(...) es una función que se utiliza para crear un nuevo objeto de estudio. Un estudio en Optuna es un contenedor para todas las pruebas (trials) que se realizarán durante la optimización. Tiene dos parámetros: 1. study_name='best_features_and_hyperparameters': Este es el nombre que se le asigna al estudio. Puedes usar este nombre para identificar el estudio en la base de datos de Optuna, especialmente si estás guardando resultados o usando la interfaz de visualización. 2. direction='maximize': Este parámetro indica que el objetivo de la optimización es maximizar el valor que se devolverá en la función objetivo (en este caso, la precisión del modelo). Si en lugar de maximizar quisieras minimizar un valor (como la pérdida), usarías direction='minimize'.\n",
    "study.optimize(objective, n_trials=10) # study.optimize(...): es un método que ejecuta el proceso de optimización, llamando repetidamente a la función objetivo para encontrar la mejor combinación de hiperparámetros y características. Tiene dos parámetros: 1. objective: Este es el nombre de la función objetivo que se definió anteriormente. Optuna la llamará múltiples veces con diferentes configuraciones de hiperparámetros, que se generan automáticamente. 2. n_trials=50: Este parámetro indica el número total de pruebas (trials) que se realizarán. En cada prueba, Optuna selecciona un conjunto diferente de hiperparámetros y características que serán probados en el modelo. En este caso, se realizarán 50 pruebas diferentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters:  {'n_estimators': 623, 'max_depth': 12, 'k_best': 12}\n",
      "Selected features:  Index(['HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP',\n",
      "       'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Extracting the best hyperparameters and selected features\n",
    "print('Best hyperparameters: ', study.best_params) # study.best_params: Este atributo contiene un diccionario con los mejores hiperparámetros que Optuna encontró durante el proceso de optimización. Los hiperparámetros se refieren a los valores óptimos que maximizan el rendimiento del modelo según la función objetivo definida.\n",
    "best_trial = study.best_trial # study.best_trial: Este atributo proporciona acceso al trial (prueba) que produjo el mejor resultado durante la optimización. Incluye información sobre los hiperparámetros utilizados, el valor de la función objetivo (por ejemplo, la precisión del modelo), y otros metadatos relevantes.\n",
    "selected_features = SelectKBest(score_func=f_classif, \n",
    "                                k=best_trial.params['k_best']).fit(X_train, y_train) # SelectKBest: Esta clase se utiliza para seleccionar las k mejores características según una función de puntuación. En este caso, f_classif es la función de puntuación que evalúa la relación entre las características y la variable objetivo.\n",
    "\n",
    "                                # SelectKBest: Esta clase se utiliza para seleccionar las k mejores características según una función de puntuación. En este caso, f_classif es la función de puntuación que evalúa la relación entre las características y la variable objetivo.\n",
    "                                \n",
    "                                # k=best_trial.params['k_best']: Aquí, se utiliza el valor de k_best que se encontró durante la optimización como el número de características a seleccionar. Este valor se extrae del mejor trial.\n",
    "                                \n",
    "                                # .fit(X_train, y_train): Este método ajusta el selector de características a los datos de entrenamiento, calculando las puntuaciones de las características y seleccionando las mejores.\n",
    "\n",
    "print('Selected features: ', X.columns[selected_features.get_support()]) # selected_features.get_support(): Este método devuelve un array booleano que indica qué características han sido seleccionadas por el SelectKBest. Devuelve True para las características seleccionadas y False para las que no lo son.\n",
    "\n",
    "# X.columns[...]: Aquí se utilizan los índices booleanos para filtrar las columnas de X. Esto devuelve solo las columnas (características) que han sido seleccionadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Submission Data\n",
    "submission_data = pd.read_csv('test.csv')\n",
    "\n",
    "# Drop Rows with Missing Values\n",
    "submission_data.fillna(0, inplace=True)\n",
    "\n",
    "# Applying the label encoders to the submission data\n",
    "for column, encoder in oe.items():\n",
    "    submission_data[column] = encoder.transform(submission_data[column].values.reshape(-1, 1))\n",
    "    \n",
    "# Generate Sample Submission\n",
    "best_clf = best_trial.user_attrs['classifier']\n",
    "y_pred = best_clf.predict(submission_data)\n",
    "submission_data['Transported'] = y_pred\n",
    "\n",
    "# Only keep required columns for submission\n",
    "submission_data = submission_data[['PassengerId', 'Transported']]\n",
    "\n",
    "# Save the DataFrame to Kaggle Directory and Upload to Submit\n",
    "submission_data.to_csv('submission.csv', index=False, header=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
